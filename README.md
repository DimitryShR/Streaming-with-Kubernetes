## Описание 

В основе проекта лежит микросервисная архитектура с 3 непрерывно работающими сервисами (оркестрация Kubernetes) обеспечивающая доставку данных от 2 источников (поток данных из Kafka с обогащением данными из key-value хранилища Redis)

## Что сделано

1. Спроектировано и реализовано DWH в Postgres:
    - stg
    - dds (Data Vault 2.0)
    - cmd
2. Реализован пайплайн доставки данных из Kafka с обогащением данными из Redis с использованием Spark Structured Streaming в stg-слой хранилища PostgreSQL и отправки данных в Kafka для следующего сервиса
2. Реализован пайплайн доставки данных из Kafka с использованием Spark Structured Streaming в dds-слой хранилища PostgreSQL (модель Data Vault 2.0) и отправки данных в Kafka для следующего сервиса
3. Реализован пайплайн доставки данных из Kafka с использованием Spark Structured Streaming в cdm-слой хранилища PostgreSQL (2 витрины)

![Архитектура проекта](Архитектура%20проекта.png)

## Структура проекта
- sql - скрипты инициализации схем и таблиц в них для слоев stg, dds, cdm
- solution\service_{xxx} - директория сервиса заливки данных в {xxx} слой, на основании которой создается контейнер с последующей отправкой в реджистри
- solution\service_{xxx}\src\\{xxx}_loader - логика чтения, обработки и заливки данных в {xxx} слой
- solution\service_{xxx}\app.py - таск для Kubernetes на запуск и поддержание работы сервиса